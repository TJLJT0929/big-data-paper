---
title: 统计学习方法--第15章 奇异值分解
---

### 1. 奇异值分解定义与性质

&emsp;&emsp;奇异值分解（Singular Value Decomposition, SVD）是线性代数中最基本、最重要的矩阵分解方法之一，在统计学、信号处理、自然语言处理等领域均有广泛应用。主成分分析（PCA）和潜在语义分析（LSA/LSI）的核心数学工具均为 SVD。

- SVD 是矩阵分解方法，其分解所得的 $U$、$V$ 均为正交矩阵。另一种常见的矩阵分解方法是非负矩阵分解（Non-negative Matrix Factorization, NMF），其特点是分解的矩阵元素非负，具有可解释性优势。
- 奇异值分解是在平方损失（Frobenius 范数）意义下对矩阵的**最优低秩近似**，本质上是一种**数据压缩**手段。
- 任意给定一个实矩阵，其奇异值分解**一定存在**，但并不唯一。$\mathit{\Sigma}$（奇异值矩阵）是唯一的，$U$ 和 $V$ 在对应的奇异值具有重数（multiplicity）时是不唯一的。
- 奇异值分解有明确的几何意义：任意线性变换都可以分解为旋转（或反射）、缩放、再旋转（或反射）三步操作。

#### 1.1 定义

&emsp;&emsp;矩阵的奇异值分解是指将 $m\times n$ 实矩阵 $A$ 表示为以下三个实矩阵乘积形式的运算

$$
A=U\mathit{\Sigma} V^\mathrm{T}
$$

其中：

- $U \in \mathbf{R}^{m \times m}$ 为 $m$ 阶正交矩阵，称为**左奇异矩阵**，其列向量 $u_1, u_2, \dots, u_m$ 称为**左奇异向量**；
- $V \in \mathbf{R}^{n \times n}$ 为 $n$ 阶正交矩阵，称为**右奇异矩阵**，其列向量 $v_1, v_2, \dots, v_n$ 称为**右奇异向量**；
- $\mathit{\Sigma} \in \mathbf{R}^{m \times n}$ 为矩形对角矩阵，其对角线元素称为**奇异值**。

基本约束条件：

- 正交性：$UU^\mathrm{T}=U^\mathrm{T}U=I_m,\quad VV^\mathrm{T}=V^\mathrm{T}V=I_n$
- 对角矩阵：$\mathit{\Sigma}=\text{diag}(\sigma_1,\sigma_2,\dots,\sigma_p),\quad p=\min(m,n)$
- 奇异值降序排列：$\sigma_1 \geq \sigma_2 \geq \dots \geq \sigma_p \geq 0$

**定理 1（奇异值分解基本定理）**：设 $A$ 为任意 $m \times n$ 实矩阵，则 $A$ 的奇异值分解存在，即存在 $m$ 阶正交矩阵 $U$、$n$ 阶正交矩阵 $V$ 和 $m \times n$ 矩形对角矩阵 $\mathit{\Sigma}$（对角线元素非负且降序排列），使得

$$
A=U\mathit{\Sigma} V^\mathrm{T}
$$

**证明思路**：考虑对��半正定矩阵 $A^\mathrm{T}A \in \mathbf{R}^{n \times n}$。由谱定理，$A^\mathrm{T}A$ 存在正交对角化 $A^\mathrm{T}A = V \Lambda V^\mathrm{T}$，其中 $\Lambda = \text{diag}(\lambda_1, \dots, \lambda_n)$，$\lambda_i \geq 0$。令 $\sigma_i = \sqrt{\lambda_i}$，对 $\sigma_i > 0$ 的部分定义 $u_i = \frac{1}{\sigma_i}Av_i$，可以验证 $\{u_i\}$ 是标准正交系。将其扩充为 $\mathbf{R}^m$ 的标准正交基即可得到完整的 $U$，从而完成证明。 $\blacksquare$

#### 1.2 紧奇异值分解与截断奇异值分解

&emsp;&emsp;上文给出的是**完全奇异值分解**（Full SVD），实际常用其紧凑形式和截断形式，用于对矩阵数据进行压缩。设实矩阵 $A \in \mathbf{R}^{m \times n}$，$\text{rank}(A) = r$，$r \leq \min(m, n)$。

- **完全奇异值分解**（Full SVD）：

$$
A=U \mathit{\Sigma} V^{\mathrm{T}}
$$

其中，$U \in \mathbf{R}^{m \times m}$，$V \in \mathbf{R}^{n \times n}$，$\mathit{\Sigma} \in \mathbf{R}^{m \times n}$。存储空间为 $O(m^2 + n^2 + mn)$。

- **紧奇异值分解**（Compact/Thin SVD，等秩分解，无损压缩）：

$$
A=U_r\mathit{\Sigma}_r V_r^{\mathrm{T}}
$$

其中，$U_r \in \mathbf{R}^{m\times r}$，$V_r \in \mathbf{R}^{n\times r}$，$\mathit{\Sigma}_r \in \mathbf{R}^{r \times r}$ 为 $r$ 阶对角矩阵。$U_r$ 对应完全奇异值分解中 $U$ 的前 $r$ 列，$V_r$ 对应 $V$ 的前 $r$ 列，$\mathit{\Sigma}_r$ 对应 $\mathit{\Sigma}$ 的前 $r$ 个正奇异值。此分解是**精确的**（$A = U_r \Sigma_r V_r^\mathrm{T}$），因为对应零奇异值的项对乘积无贡献。存储空间为 $O(mr + nr + r)$。

- **截断奇异值分解**（Truncated SVD，降秩分解，有损压缩）：

$$
A \approx U_k\mathit{\Sigma}_k V_k^{\mathrm{T}}, \quad k < r
$$

其中，$U_k \in \mathbf{R}^{m\times k}$，$V_k \in \mathbf{R}^{n\times k}$，$\mathit{\Sigma}_k \in \mathbf{R}^{k \times k}$ 为 $k$ 阶对角矩阵。保留前 $k$ 个最大奇异值及其对应的奇异向量。此分解是**近似的**，近似误差由舍弃的奇异值决定（见第 3 节）。存储空间降至 $O((m+n)k)$，当 $k \ll \min(m,n)$ 时实现了显著的数据压缩。

**Remark**：紧 SVD 与截断 SVD 的本质区别在于——前者保留了所有非零奇异值，因而是精确分解；后者仅保留前 $k$ 个最大奇异值，是有损近似。在实际应用中，截断 SVD 是使用最频繁的形式。

#### 1.3 几何解释

&emsp;&emsp;矩阵 $A \in \mathbf{R}^{m\times n}$ 表示了一个从 $n$ 维空间 $\mathbf{R}^n$ 到 $m$ 维空间 $\mathbf{R}^m$ 的**线性变换**

$$
T: x \rightarrow Ax, \quad x \in \mathbf{R}^n, \quad Ax \in \mathbf{R}^m
$$

根据 $A = U\Sigma V^\mathrm{T}$，该线性变换可以分解为三个简单的变换的复合：

1. **坐标系的旋转或反射变换** $V^\mathrm{T}$：将 $\mathbf{R}^n$ 中的向量 $x$ 在标准正交基 $\{v_1, \dots, v_n\}$ 下表示，即 $y = V^\mathrm{T}x$；
2. **坐标轴的缩放变换** $\mathit{\Sigma}$：对各坐标轴分别进行缩放，第 $i$ 个坐标乘以 $\sigma_i$，即 $z = \Sigma y$；
3. **坐标系的旋转或反射变换** $U$：将缩放后的向量通过正交变换 $U$ 映射到 $\mathbf{R}^m$ 中，即 $Ax = Uz$。

&emsp;&emsp;奇异值分解基本定理保证了这种分解一定存在。几何上，$V^\mathrm{T}$ 将 $\mathbf{R}^n$ 中的单位超球面映射为单位超球面（保持形状不变），$\Sigma$ 将其变为超椭球面（各轴半径为 $\sigma_i$），$U$ 再将超椭球面旋转到最终的位置。因此，**任何线性变换都将单位超球面映射为超椭球面，奇异值就是该超椭球面各主轴的半径长度**。

#### 1.4 主要性质

**性质 1**：$A^\mathrm{T}A$ 和 $AA^\mathrm{T}$ 的特征分解存在，且可由矩阵 $A$ 的奇异值分解直接给出。

$$
A^\mathrm{T}A = (U\Sigma V^\mathrm{T})^\mathrm{T}(U\Sigma V^\mathrm{T}) = V(\Sigma^\mathrm{T}\Sigma)V^\mathrm{T}
$$

$$
AA^\mathrm{T} = (U\Sigma V^\mathrm{T})(U\Sigma V^\mathrm{T})^\mathrm{T} = U(\Sigma\Sigma^\mathrm{T})U^\mathrm{T}
$$

其中 $\Sigma^\mathrm{T}\Sigma = \text{diag}(\sigma_1^2, \sigma_2^2, \dots, \sigma_n^2) \in \mathbf{R}^{n \times n}$，$\Sigma\Sigma^\mathrm{T} = \text{diag}(\sigma_1^2, \sigma_2^2, \dots, \sigma_m^2) \in \mathbf{R}^{m \times m}$。因此，$A^\mathrm{T}A$ 的特征值为 $\sigma_i^2$，特征向量为 $v_i$（右奇异向量）；$AA^\mathrm{T}$ 的特征值为 $\sigma_i^2$，特征向量为 $u_i$（左奇异向量）。

**性质 2**：奇异值、左奇异向量、右奇异向量之间的关系

$$
Av_i = \sigma_i u_i, \quad i = 1, 2, \dots, r
$$

$$
A^\mathrm{T}u_i = \sigma_i v_i, \quad i = 1, 2, \dots, r
$$

写成矩阵形式为 $AV = U\Sigma$，$A^\mathrm{T}U = V\Sigma^\mathrm{T}$。

**性质 3**：矩阵 $A$ 的奇异值是唯一的，但矩阵 $U$ 和 $V$ 一般不唯一。当所有正奇异值互不相等时，$U$ 和 $V$ 在差一个符号的意义下唯一。

**性质 4**：$\text{rank}(A) = \text{rank}(\Sigma) = r$，即矩阵的秩等于正奇异值的个数。

**性质 5（基本子空间）**：设 $\text{rank}(A) = r$，则 SVD 给出了 $A$ 的四个基本子空间的标准正交基：

| 子空间 | 维数 | 标准正交基 |
|:---:|:---:|:---:|
| 列空间 $\mathcal{R}(A)$ | $r$ | $u_1, u_2, \dots, u_r$ |
| 左零空间 $\mathcal{N}(A^\mathrm{T})$ | $m - r$ | $u_{r+1}, u_{r+2}, \dots, u_m$ |
| 行空间 $\mathcal{R}(A^\mathrm{T})$ | $r$ | $v_1, v_2, \dots, v_r$ |
| 零空间 $\mathcal{N}(A)$ | $n - r$ | $v_{r+1}, v_{r+2}, \dots, v_n$ |

这是 SVD 在线性代数中最深刻的性质之一，它揭示了 SVD 与矩阵的**四个基本子空间**（Strang 的"线性代数基本定理"）之间的紧密联系。

**性质 6**：矩阵的 $2$-范数（谱范数）和 $F$-范数均可通过奇异值表示：

$$
\|A\|_2 = \sigma_1, \quad \|A\|_F = \left(\sum_{i=1}^{\min(m,n)} \sigma_i^2\right)^{1/2}
$$

**性质 7**：矩阵的**条件数**（condition number）为

$$
\kappa(A) = \frac{\sigma_1}{\sigma_r}
$$

条件数刻画了矩阵的数值稳定性，$\kappa(A)$ 越大，矩阵越"病态"（ill-conditioned），数值计算越不稳定。

### 2. 奇异值分解的计算

&emsp;&emsp;以下是基于特征值分解的经典 SVD 计���步骤（适合理论推导与小规模矩阵）：

1. **求 $A^\mathrm{T}A$ 的特征值和特征向量**

   计算对称半正定矩阵 $W = A^\mathrm{T}A$。

   求解特征方程：

   $$
   (W - \lambda I)x = 0
   $$

   得到特征值 $\lambda_i$，并将特征值由大到小排列：

   $$
   \lambda_1 \geq \lambda_2 \geq \cdots \geq \lambda_n \geq 0
   $$

   将特征值 $\lambda_i\ (i = 1, 2, \cdots, n)$ 代入特征方程求得对应的特征向量。

2. **求 $n$ 阶正交矩阵 $V$**

   将特征向量正交化（若存在重特征值，需使用 Gram–Schmidt 正交化）、单位化，得到单位特征向量 $v_1, v_2, \cdots, v_n$，构成 $n$ 阶正交矩阵 $V$：

   $$
   V = \begin{bmatrix} v_1 & v_2 & \cdots & v_n \end{bmatrix}
   $$

3. **求 $m \times n$ 对角矩阵 $\Sigma$**

   计算 $A$ 的奇异值：

   $$
   \sigma_i = \sqrt{\lambda_i}, \quad i = 1, 2, \cdots, n
   $$

   构造 $m \times n$ 矩形对角矩阵 $\Sigma$，主对角线元素是奇异值，其余元素为零：

   $$
   \Sigma = \text{diag}(\sigma_1, \sigma_2, \cdots, \sigma_n) \in \mathbf{R}^{m \times n}
   $$

4. **求 $m$ 阶正交矩阵 $U$**

   对 $A$ 的前 $r$ 个正奇异值，令：

   $$
   u_j = \frac{1}{\sigma_j}Av_j, \quad j = 1, 2, \cdots, r
   $$

   得到：

   $$
   U_1 = \begin{bmatrix} u_1 & u_2 & \cdots & u_r \end{bmatrix}
   $$

   求 $A^\mathrm{T}$ 的零空间（即 $AA^\mathrm{T}$ 对应零特征值的特征向量）的一组标准正交基 $\{u_{r+1}, u_{r+2}, \cdots, u_m\}$，令：

   $$
   U_2 = \begin{bmatrix} u_{r+1} & u_{r+2} & \cdots & u_m \end{bmatrix}
   $$

   并令

   $$
   U = \begin{bmatrix} U_1 & U_2 \end{bmatrix}
   $$

5. **得到奇异值分解**

   $$
   A = U \Sigma V^\mathrm{T}
   $$

**Remark**：上述通过显式构造 $A^\mathrm{T}A$ 来计算 SVD 的方法在理论上是正确的，但在数值计算中并不推荐，因为 $A^\mathrm{T}A$ 的条件数是 $A$ 的条件数的平方，会导致数值精度的严重损失。实际计算中的高效数值算法见第 6 节。

### 3. 奇异值分解与矩阵近似

#### 3.1 弗罗贝尼乌斯范数

&emsp;&emsp;奇异值分解也是一种矩阵近似的方法，这个近似是在**弗罗贝尼乌斯范数**（Frobenius norm）意义下的近似。

$$
\|A\|_F = \left(\sum_{i=1}^m\sum_{j=1}^n a_{ij}^2\right)^{1/2} = \left(\text{tr}(A^\mathrm{T}A)\right)^{1/2}, \quad A \in \mathbf{R}^{m\times n}
$$

弗罗贝尼乌斯范数是向量的 $L_2$ 范数对矩阵的自然推广，在机器学习中对应于**平方损失函数**。

**命题**：设矩阵 $A \in \mathbf{R}^{m\times n}$，奇异值分解为 $A=U\Sigma V^\mathrm{T}$，其中 $\Sigma = \text{diag}(\sigma_1,\sigma_2,\cdots,\sigma_p)$，$p=\min(m,n)$，则

$$
\|A\|_F = \left(\sigma_1^2 + \sigma_2^2 + \cdots + \sigma_p^2\right)^{1/2}
$$

**证明**：利用正交不变性，$\|A\|_F = \|U\Sigma V^\mathrm{T}\|_F = \|\Sigma\|_F$（正交变换不改变 Frobenius 范数），而 $\|\Sigma\|_F$ 就是对角元素的平方和的平方根。 $\blacksquare$

#### 3.2 矩阵的最优近似

&emsp;&emsp;奇异值分解在平方损失（Frobenius 范数）意义下给出了矩阵的**最优低秩近似**，这正是 SVD 用于数据压缩的理论基础。

**定理 2（Eckart–Young–Mirsky 定理，1936）**：设 $A \in \mathbf{R}^{m \times n}$，$\text{rank}(A) = r$，奇异值分解为 $A = U\Sigma V^\mathrm{T}$。设 $\mathcal{M}_k$ 为 $\mathbf{R}^{m\times n}$ 中所有秩不超过 $k$（$0 < k < r$）的矩阵集合。则在 Frobenius 范数意义下，$A$ 在 $\mathcal{M}_k$ 上的最优近似为其截断奇异值分解

$$
A_k = U_k \Sigma_k V_k^\mathrm{T} = \sum_{i=1}^k \sigma_i u_i v_i^\mathrm{T}
$$

且近似误差为

$$
\min_{S \in \mathcal{M}_k} \|A - S\|_F = \|A - A_k\|_F = \left(\sigma_{k+1}^2 + \sigma_{k+2}^2 + \cdots + \sigma_r^2\right)^{1/2}
$$

**Remark 1**：该定理不仅对 Frobenius 范数成立，对谱范数（$2$-范数）同样成立：

$$
\min_{S \in \mathcal{M}_k} \|A - S\|_2 = \|A - A_k\|_2 = \sigma_{k+1}
$$

这说明截断 SVD 是一种**同时在多种范数意义下最优**的低秩近似。

**Remark 2**：近似质量可以用保留的"能量"比来衡量：

$$
\eta(k) = \frac{\sigma_1^2 + \sigma_2^2 + \cdots + \sigma_k^2}{\sigma_1^2 + \sigma_2^2 + \cdots + \sigma_r^2}
$$

当 $\eta(k)$ 接近 $1$ 时，说明前 $k$ 个奇异值捕获了矩阵的大部分信息。实际应用中常选取 $\eta(k) \geq 0.9$（或 $0.95$, $0.99$）对应的 $k$ 值。

#### 3.3 矩阵的外积展开式

&emsp;&emsp;奇异值分解可以用**外积（dyadic）展开式**表示：

$$
A = \sum_{i=1}^{r} \sigma_i u_i v_i^\mathrm{T}
$$

其中每一项 $\sigma_i u_i v_i^\mathrm{T}$ 是一个秩为 $1$ 的 $m \times n$ 矩阵。定义前 $k$ 项的部分和为

$$
A_k = \sum_{i=1}^{k} \sigma_i u_i v_i^\mathrm{T}
$$

则 $\text{rank}(A_k) = k$，且由 Eckart–Young–Mirsky 定理，$A_k$ 是秩为 $k$ 的矩阵中在 Frobenius 范数和谱范数下对 $A$ 的最优近似矩阵。

外积展开式揭示了 SVD 的**逐级逼近**本质：每增加一项 $\sigma_{k+1}u_{k+1}v_{k+1}^\mathrm{T}$，近似误差下降 $\sigma_{k+1}^2$（在 Frobenius 范数的平方意义下）。奇异值衰减得越快，低秩近似的效果越好。

### 4. 奇异值分解与主成分分析（PCA）

&emsp;&emsp;主成分分析（Principal Component Analysis, PCA）是最经典的降维方法之一，其核心计算等价于对**中心化数据矩阵**进行奇异值分解。

#### 4.1 PCA 的 SVD 表述

&emsp;&emsp;设有 $m$ 个样本、$n$ 个特征的数据矩阵 $X \in \mathbf{R}^{m \times n}$（每行为一个样本），已中心化（各列均值为零）。则样本协方差矩阵为

$$
C = \frac{1}{m-1}X^\mathrm{T}X
$$

对 $X$ 做奇异值分解 $X = U\Sigma V^\mathrm{T}$，则

$$
C = \frac{1}{m-1}V\Sigma^\mathrm{T}\Sigma V^\mathrm{T} = V\left(\frac{\Sigma^\mathrm{T}\Sigma}{m-1}\right)V^\mathrm{T}
$$

因此：

- **主成分方向**：协方差矩阵 $C$ 的特征向量就是 $V$ 的列向量（即 $A$ 的右奇异向量）；
- **主成分方差**：第 $i$ 个主成分的方差为 $\sigma_i^2 / (m-1)$；
- **主成分得分**：$m$ 个样本在前 $k$ 个主成分上的投影（降维结果）为 $Z_k = U_k \Sigma_k \in \mathbf{R}^{m \times k}$。

#### 4.2 降维与信息保留

&emsp;&emsp;选取前 $k$ 个主成分，相当于对数据矩阵做截断 SVD，保留的方差比例（信息比例）为

$$
\eta(k) = \frac{\sum_{i=1}^k \sigma_i^2}{\sum_{i=1}^r \sigma_i^2}
$$

该比例也称为**累计贡献率**（cumulative proportion of variance explained），是确定降维维度 $k$ 的常用准则。

### 5. 奇异值分解与潜在语义分析（LSA）

&emsp;&emsp;潜在语义分析（Latent Semantic Analysis, LSA），又称潜在语义索引（Latent Semantic Indexing, LSI），是信息检索领域的经典方法，由 Deerwester 等人于 1990 年提出，其核心就是对**词-文档矩阵**进行截断 SVD。

#### 5.1 基本思想

&emsp;&emsp;构建词-文档矩阵 $A \in \mathbf{R}^{m \times n}$，其中 $m$ 为词汇量，$n$ 为文档数，$a_{ij}$ 通常取词频（TF）或 TF-IDF 权重。直接使用 $A$ 进行检索存在**同义词**和**多义词**问题。

&emsp;&emsp;LSA 对 $A$ 做截断 SVD：

$$
A \approx A_k = U_k \Sigma_k V_k^\mathrm{T}
$$

将词和文档都映射到 $k$ 维的**潜在语义空间**：

- 词在潜在语义空间中的表示：$U_k \Sigma_k$ 的行向量；
- 文档在潜在语义空间中的表示：$V_k \Sigma_k$ 的行向量。

在该空间中使用余弦相似度进行文档检索和相似度计算，能有效克服词级别的稀疏性问题。

#### 5.2 与现代方法的关系

&emsp;&emsp;LSA 可以视为词嵌入（word embedding）方法的先驱。后续的 Word2Vec（Mikolov et al., 2013）、GloVe（Pennington et al., 2014）等方法在某种程度上都可以看作是 LSA 思想在神经网络框架下的推广和发展。Levy & Goldberg (2014) 证明了 Word2Vec 的 Skip-gram 模型实际上等价于对**逐点互信息（PMI）矩阵**进行隐式的矩阵分解。

### 6. 数值计算方法

&emsp;&emsp;第 2 节的经典算法通过显式计算 $A^\mathrm{T}A$ 来求解，但这在数值上是不稳定的。实际工程中采用更加精巧的算法。

#### 6.1 Golub–Kahan 双对角化

&emsp;&emsp;现代 SVD 计算的标准流程（Golub & Van Loan, 1965）分为两步：

1. **双对角化（Bidiagonalization）**：通过 Householder 变换将 $A$ 化为上双对角矩阵 $B$：

$$
U_1^\mathrm{T} A V_1 = B = \begin{bmatrix} d_1 & f_1 & & \\ & d_2 & f_2 & \\ & & \ddots & \ddots \\ & & & d_n \end{bmatrix}
$$

&emsp;&emsp;此步骤的计算复杂度为 $O(mn^2)$（设 $m \geq n$）。

2. **对角化**：对双对角矩阵 $B$ 施加隐式 QR 迭代（Golub–Kahan SVD step），将 $B$ 迭代收敛为对角矩阵。此步骤的计算复杂度为 $O(n^2)$（每次迭代），总迭代次数通常为 $O(n)$。

#### 6.2 随机化 SVD

&emsp;&emsp;对于大规模矩阵（如推荐系统中的用户-物品矩阵），精确 SVD 的计算代价过高。**随机化 SVD**（Randomized SVD, Halko et al., 2011）是一种高效的近似方法：

1. 生成随机矩阵 $\Omega \in \mathbf{R}^{n \times (k+p)}$（$p$ 为过采样参数，通常取 $5 \sim 10$）；
2. 计算 $Y = A\Omega$，对 $Y$ 做 QR 分解 $Y = QR$；
3. 计算 $B = Q^\mathrm{T}A$，对 $B$ 做精确 SVD：$B = \hat{U}\Sigma V^\mathrm{T}$；
4. 令 $U = Q\hat{U}$，得到 $A \approx U\Sigma V^\mathrm{T}$。

计算复杂度为 $O(mnk)$，远低于精确 SVD 的 $O(mn\min(m,n))$。

### 7. 奇异值分解与广义逆（Moore–Penrose 伪逆）

&emsp;&emsp;SVD 为计算**Moore–Penrose 伪逆**（广义逆）提供了直接且数值稳定的方法。

#### 7.1 定义

&emsp;&emsp;矩阵 $A \in \mathbf{R}^{m \times n}$ 的 Moore–Penrose 伪逆 $A^+ \in \mathbf{R}^{n \times m}$ 是满足以下四个条件的唯一矩阵：

1. $AA^+A = A$
2. $A^+AA^+ = A^+$
3. $(AA^+)^\mathrm{T} = AA^+$
4. $(A^+A)^\mathrm{T} = A^+A$

#### 7.2 SVD 表示

&emsp;&emsp;设 $A = U\Sigma V^\mathrm{T}$，$\text{rank}(A) = r$，则

$$
A^+ = V\Sigma^+ U^\mathrm{T}
$$

其中 $\Sigma^+ \in \mathbf{R}^{n \times m}$ 为将 $\Sigma$ 的非零对角元素取倒数、再转置所得的矩阵：

$$
\Sigma^+ = \text{diag}\left(\frac{1}{\sigma_1}, \frac{1}{\sigma_2}, \dots, \frac{1}{\sigma_r}, 0, \dots, 0\right)^\mathrm{T}
$$

#### 7.3 最小二乘问题

&emsp;&emsp;对于超定方程组 $Ax = b$（$m > n$），最小二乘解为

$$
x^* = A^+ b = V\Sigma^+ U^\mathrm{T} b
$$

当 $A$ 列满秩时，此即正规方程 $A^\mathrm{T}Ax = A^\mathrm{T}b$ 的解；当 $A$ 不满秩时，$x^* = A^+b$ 给出的是所有最小化 $\|Ax - b\|_2$ 的解中**范数最小**的那一个。这一性质在**岭回归**（Ridge Regression）和**正则化**理论中有重要应用。

### 8. 总结与进一步阅读

&emsp;&emsp;奇异值分解是线性代数和矩阵分析中的核心工具，在统计学习中具有基础性地位。本章的知识结构可以总结如下：

| 主题 | 核心内容 |
|:---:|:---|
| 定义与存在性 | SVD 基本定理保证任意实矩阵的 SVD 存在 |
| 三种形式 | 完全 SVD → 紧 SVD → 截断 SVD |
| 几何意义 | 旋转 → 缩放 → 旋转 |
| 最优近似 | Eckart–Young–Mirsky 定理 |
| PCA | 数据降维与特征提取 |
| LSA | 文本语义建模与信息检索 |
| 伪逆 | 最小二乘与正则化 |
| 数值算法 | Golub-Kahan 双对角化、随机化 SVD |

### 参考文献

1. 李航. 统计学习方法（第2版）. 清华大学出版社, 2019. 第15章.
2. Golub, G. H., & Van Loan, C. F. *Matrix Computations* (4th ed.). Johns Hopkins University Press, 2013.
3. Strang, G. *Linear Algebra and Its Applications* (4th ed.). Cengage Learning, 2006.
4. Eckart, C., & Young, G. The approximation of one matrix by another of lower rank. *Psychometrika*, 1(3), 211–218, 1936.
5. Deerwester, S., Dumais, S. T., Furnas, G. W., Landauer, T. K., & Harshman, R. Indexing by latent semantic analysis. *Journal of the American Society for Information Science*, 41(6), 391–407, 1990.
6. Halko, N., Martinsson, P. G., & Tropp, J. A. Finding structure with randomness: Probabilistic algorithms for constructing approximate matrix decompositions. *SIAM Review*, 53(2), 217–288, 2011.
7. Levy, O., & Goldberg, Y. Neural word embedding as implicit matrix factorization. *Advances in Neural Information Processing Systems (NeurIPS)*, 2177–2185, 2014.
