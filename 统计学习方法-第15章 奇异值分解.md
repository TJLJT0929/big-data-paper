---
title: 统计学习方法--第15章 奇异值分解
date: 2024-10-8
tags: 统计学习方法
---


### 1. 奇异值分解定义与性质

&emsp;&emsp;SVD 是线性代数的概念，在统计学中有广泛应用，PCA 和 LSA 中都有应用。

- SVD是矩阵分解方法，特点是分解的矩阵正交。另一种矩阵分解方法叫做 NMF，其特点是分解的矩阵非负。
- 奇异值分解是在平方损失意义下对矩阵的最优近似，即**数据压缩**。
- 任意给定一个实矩阵，其奇异值分解一定存在，但并不唯一。$\mathit{\Sigma}$ 是唯一的，$U$ 和 $V^\mathrm{T}$ 是可变的。
- 奇异值分解有明确的几何意义

#### 1.1 定义

&emsp;&emsp;矩阵的奇异值分解是指将 $m\times n$ 实矩阵 $A$ 表示为以下三个实矩阵乘积形式的运算

$$
A=U\mathit{\Sigma} V^\mathrm T
$$

$U,V$ 分别是 m 阶、n 阶正交矩阵（左、右奇异向量）， $\mathit{\Sigma}$ 是 $m \times n$ 的对角矩阵

- 正交：$UU^T=I,VV^T$
- 对角矩阵：$\mathit{\Sigma}=diag(\sigma_1,\dots,\sigma_p)$
- 奇异值： $\sigma_1 \geq \dots \geq \sigma_p \geq 0$
- 维数： $p=min(m,n)$

**理论保证（奇异值分解基本定理）**：若A为实矩阵，则A的奇异值分解存在
$$
A=U\mathit{\Sigma} V^\mathrm T
$$

其中 $U,V,\mathit{\Sigma}$ 分别为 $m$ 阶正交矩阵、$n$ 阶正交矩阵、$m \times n$ 的对角矩阵，对角线元素非负且降序排列。

#### 1.2 紧奇异值分解与截断奇异值分解

&emsp;&emsp;上文给出的是完全奇异值分解，实际常用其紧凑（等秩分解，无损压缩）、截断形式（降秩分解，有损压缩），用于对矩阵数据进行压缩。设实矩阵 $A_{m \times n},rank(A)=r,r\leq min(m,n)$

- 完全奇异值分解：

$$
A=U \mathit{\Sigma} V^{\mathrm{T}}
$$

其中，$U,V,\mathit{\Sigma}$ 分别为 m 阶正交矩阵、n 阶正交矩阵、$m \times n$ 的对角矩阵

- 紧奇异值分解：

$$
A=U_r\mathit\Sigma_r V_r^{\mathrm{T}}
$$

其中，$U_r,V_r,\mathit{\Sigma}_r$ 分别为 $m\times r$ 矩阵、$n\times r$ 矩阵、$r$ 阶对角矩阵。对应完全奇异值分解的 $U$ 的前 $r$ 列、$V$ 的前 $r$ 列、$\mathit{\Sigma}$ 的前 $r$ 个对角线元素。

- 截断奇异值分解：

$$
A \approx U_k\mathit{\Sigma_k} V_k^{\mathrm{T}}
$$

其中，$U_k,V_k,\mathit{\Sigma}_k$ 分别为 $m\times k$ 矩阵、$n\times k$ 矩阵、$k$ 阶对角矩阵。对应完全奇异值分解的 $U$ 的前 $k$ 列、$V$ 的前 $k$ 列、$\mathit{\Sigma}$ 的前 $k$ 个对角线元素。

#### 1.3 几何解释

&emsp;&emsp;$A_{m\times n}$ 表示了一个从 $n$ 维空间 $\mathbf{R}^n$ 到 $m$ 维空间 $\mathbf{R}^m$ 的一个**线性变换**

$$
T:x\rightarrow Ax\\
x\in\mathbf{R}^n\\
Ax\in \mathbf{R}^m
$$

线性变换可以分解为三个简单的变换：

- 坐标系的旋转或反射变换，$V^\mathrm{T}$
- 坐标轴的缩放变换，$\mathit{\Sigma}$
- 坐标系的旋转或反射变换，$U$

&emsp;&emsp;奇异值定理则保证这种分解一定存在。任意一个向量 $x\in\mathbf{R}^n$ ，经过坐标系的旋转或者反射变换 $V^T$ ，坐标轴的缩放变换 $\Sigma$ ，坐标系的旋转或反射变换 $U$，得到向量 $Ax\in \mathbf{R}^m$.

#### 1.4 主要性质

- $AA^\mathrm{T}$ 和 $A^\mathrm{T}A$ 的特征分解存在，且可由矩阵 $A$ 的奇异值分解的矩阵表示

$$
A^TA=(U \mathit{\Sigma} V^{\mathrm{T}})^T(U \mathit{\Sigma} V^{\mathrm{T}})=V(\Sigma^T\Sigma)V^T\\
AA^T=(U \mathit{\Sigma} V^{\mathrm{T}})(U \mathit{\Sigma} V^{\mathrm{T}})^T=U(\Sigma^T\Sigma)U^T
$$

- 奇异值，左奇异向量，右奇异向量之间的关系

$$
AV=U\Sigma\\
A^TU=V\Sigma
$$

- 矩阵 $A$ 的奇异值分解中，奇异值是唯一的，但是矩阵 $U$ 和 $V$ 不是唯一的
- 矩阵 $A$ 和 $\Sigma$ 等秩，等于奇异值个数
- 矩阵 $A$ 的 $r$ 个右奇异向量 $v_1,\dotsb,v_r$ 构成 $A^T$ 的值域 $R(A^T)$ 一组标准正交基， $n-r$ 个右奇异向量 $v_{r+1},\dotsb,v_n$ 构成 $A$ 的零空间 $N(A)$ 的一组标准正交基。
- 矩阵 $A$ 的 $r$ 个左奇异向量 $u_1,\dotsb,u_r$ 构成值域 $R(A)$ 一组标准正交基， $m-r$ 个左奇异向量 $u_{r+1},\dotsb,u_m$ 构成 $A^T$ 的零空间 $N(A^T)$ 的一组标准正交基。

### 2. 奇异值分解的计算

1. **首先求 $A^TA$ 的特征值和特征向量**。

   计算对称矩阵 $W = A^TA$。

   求解特征方程：

   $$
   (W - \lambda I )x = 0
   $$

   得到特征值 $\lambda_i$，并将特征值由大到小排列：

   $$
   \lambda_1 \geq \lambda_2 \geq \cdots \geq \lambda_n \geq 0
   $$

   将特征值 $\lambda_i \ (i = 1, 2, \cdots, n)$ 代入特征方程求得对应的特征向量。
2. **求 $n$ 阶正交矩阵 $V$**

   将特征向量单位化，得到单位特征向量 $v_1, v_2, \cdots, v_n$，构成 $n$ 阶正交矩阵 $V$：

   $$
   V = \begin{bmatrix} v_1 & v_2 & \cdots & v_n \end{bmatrix}
   $$
3. **求 $m \times n$ 对角矩阵 $\Sigma$**

   计算 $A$ 的奇异值：

   $$
   \sigma_i = \sqrt{\lambda_i}, \ i = 1, 2, \cdots, n
   $$

   构造 $m \times n$ 矩形对角矩阵 $\Sigma$，主对角线元素是奇异值，其余元素为零：

   $$
   \Sigma = \text{diag}(\sigma_1, \sigma_2, \cdots, \sigma_n)
   $$
4. **求 $m$ 阶正交矩阵 $U$**

   对 $A$ 的前 $r$ 个正奇异值，令：

   $$
   u_j = \frac{1}{\sigma_j}Av_j, \quad j = 1, 2, \cdots, r
   $$

   得到：

   $$
   U_1 = \begin{bmatrix} u_1 & u_2 & \cdots & u_r \end{bmatrix}
   $$

   求 $A^T$ 的零空间的一组标准正交基 $\{u_{r+1}, u_{r+2}, \cdots, u_m\}$，令：

   $$
   U_2 = \begin{bmatrix} u_{r+1} & u_{r+2} & \cdots & u_m \end{bmatrix}
   $$

   并令

   $$
   U = \begin{bmatrix} U_1 & U_2 \end{bmatrix}
   $$
5. **得到奇异值分解**

   $$
   A = U \Sigma V^T
   $$

### 3. 奇异值分解与矩阵近似

#### 3.1 弗罗贝尼乌斯范数

&emsp;&emsp;奇异值分解也是一种矩阵近似的方法，这个近似是在**弗罗贝尼斯范数**意义下的近似。
$$
\left\|A\right\|_F=\left(\sum_{i=1}^m\sum_{j=1}^n a_{ij}^2\right)^\frac{1}{2},
A\in\mathbf{R}^{m\times n}, A=[a_{ij}]_{m\times n}
$$

矩阵的弗罗贝尼斯范数是向量的 $L_2$ 范数的直接推广，对应着机器学习里面的平方损失函数。

设矩阵 $A\in\mathbf{R}^{m\times n}$ ，奇异值分解为 $A=U\mathit\Sigma V^\mathrm{T}$，其中 $\mathit{\Sigma}\rm=diag(\sigma_1,\sigma_2,\cdots,\sigma_n)$ ，那么有
$$
\it\left\|A\right\|_F\rm=(\sigma_1^2+\sigma_2^2+\cdots+\sigma_n^2)^\frac{1}{2}
$$

#### 3.2 矩阵的最优近似

&emsp;&emsp;奇异值分解是在平方损失（F 范数）意义下对矩阵的最优近似，即数据压缩。设 $\mathcal{M}$ 为 $\mathbf{R}^{m\times n}$ 中所有秩不超过 $k，0<k<r$ 的矩阵集合，若秩为 $k$ 的矩阵 $X\in \mathcal{M}$ 满足

$$
\left\|A-X\right\|_F=\min_{S\in \mathcal{M}}\left\|A-S\right\|_F
$$

则

$$
\left\|A-X\right\|_F=(\sigma_{k+1}^2+\sigma_{k+2}^2+\cdots+\sigma_n^2)^\frac{1}{2}
$$

#### 3.3 矩阵的外积展开式

&emsp;&emsp;下面利用外积展开式对矩阵 $A$ 的近似，奇异值分解可以用外积形式表示。

$$
\begin{aligned}
A&=\sigma_1u_1v_1^\mathrm{T}+\sigma_2u_2v_2^\mathrm{T}+\cdots+\sigma_nu_nv_n^\mathrm{T}\\
&=\sum_{k=1}^nA_k\\
&=\sum_{k=1}^n\sigma_ku_kv_k^\mathrm{T}
\end{aligned}
$$

其中，$u_kv_k^\mathrm{T}$ 为 $m\times n$ 矩阵。设矩阵 $A_k$

$$
A_k=\sigma_1u_1v_1^\mathrm{T}+\sigma_2u_2v_2^\mathrm{T}+\cdots+\sigma_ku_kv_k^\mathrm{T}
$$

则 $A_k$ 的秩为 $k$ ，并且 $A_k$ 是秩为 $k$ 的矩阵中在 $F$ 范数下 $A$ 的最优近似矩阵，也就是 $A$ 截断奇异值分解。
